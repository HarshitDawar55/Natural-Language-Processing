{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2bcb9af",
   "metadata": {},
   "source": [
    "# Topic Modelling by `Mr. Harshit Dawar!`\n",
    "### Algorithm: LDA (Latent Dirichlet Allocation)\n",
    "\n",
    "***Steps***\n",
    "1. A random number of topics will be decided by the user to which the words from the document will be assigned.\n",
    "2. Each word from each document will be assigned to any of the random topics initially.\n",
    "3. Now, random topics from each document & words assignments to those topics in each document will be obtained. Although, initial assignment will not make any sense.\n",
    "4. Steps 2 & 3 will be repeated until the best assignments are provided using the formula given below.\n",
    "\n",
    "For each topic:  ***probability( topic \"t\" | document \"d\")*** <= Probability of topic \"t\" existing in document \"d\".\n",
    "\n",
    "For each word:  ***probability( word \"w\" | topic \"t\")*** <= Probability of word \"w\" belonging to topic \"t\".\n",
    "\n",
    "Final probability that a topic \"t\" generated word \"w\" in document \"d\": ***probability( topic \"t\" | document \"d\") * probability( word \"w\" | topic \"t\")***\n",
    "\n",
    "\n",
    "**Important Pointers**\n",
    "\n",
    "* The user has to decide the number of topics to get from the document\n",
    "* The user has to interpret the topics itself.\n",
    "\n",
    "**Few Assumptions of LDA**\n",
    "\n",
    "* Documents are probability distributions over Topics, Topics are probabilty distributions over words.\n",
    "* Documents with similar topics uses similar groups of words.\n",
    "* Topics can be founded by searching for the words that occur across the corpus in documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0210e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb35011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f22c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the Washington of 2016, even when the polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump has used Twitter  —   his prefe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump is unabashedly praising Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Updated at 2:50 p. m. ET, Russian President Vl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From photography, illustration and video, to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  In the Washington of 2016, even when the polic...\n",
       "1    Donald Trump has used Twitter  —   his prefe...\n",
       "2    Donald Trump is unabashedly praising Russian...\n",
       "3  Updated at 2:50 p. m. ET, Russian President Vl...\n",
       "4  From photography, illustration and video, to d..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036fde94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11992, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84022d",
   "metadata": {},
   "source": [
    "## Getting the word vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c218bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* min_df represents min. number of documents in which a word should occur. A word with a number below this\n",
    "  will be ignored.\n",
    "* max_df represents max. word frequency of occurence of a word in the document above which all the\n",
    "  words will be ignored.\n",
    "  \n",
    "* Stopwrods of English will be removed.\n",
    "\"\"\"\n",
    "vectorizer = CountVectorizer(min_df = 2, max_df = 0.95, stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bead3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_word_matrix = vectorizer.fit_transform(data.Article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de9212d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11992x54777 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3033388 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_word_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6c0c354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  9, 10, 15, 19])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(document_word_matrix.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010aae47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
