{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd4e2e4",
   "metadata": {},
   "source": [
    "# Bag Of Words by `Mr. Harshit Dawar!`\n",
    "* It is a technique to transform the words into differnet numbers, then make the sentences into the matrix form having the word count for each word in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfab4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Libraries\n",
    "\n",
    "import goose3     # For fetching the text from the URL\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee0b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "goose = goose3.Goose()\n",
    "\n",
    "# Extracting data from the wikopedia article on NLP!\n",
    "data = goose.extract(\"https://en.wikipedia.org/wiki/Natural_language_processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17fd6727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the text data by extracting it from the data Variable!\n",
    "data = data.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ca87e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural languag'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0b762d",
   "metadata": {},
   "source": [
    "## Generating sentences from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e960a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence for sentence in nltk.sent_tokenize(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52141152",
   "metadata": {},
   "source": [
    "## Creating a Bag Of Words Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08ac6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_Vectorizer = CountVectorizer()\n",
    "Vectorized_Data = Word_Vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "332a4545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorized_Data.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74cfef06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out first sentence transformed into vectors!\n",
    "Vectorized_Data.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1014e010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '19', '1950', '1950s', '1954', '1960s', '1964', '1966', '1970s', '1975', '1976', '1977', '1978', '1979', '1980s', '1981', '1990s', '1999', '20', '2000s', '2002', '2003', '2006', '2007', '2009', '2010s', '2012', '2015', '2017', '2018', '2020', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '60', 'abandoned', 'able', 'about', 'above', 'accidentally', 'accurate', 'accurately', 'achieve', 'acl', 'acquiring', 'act', 'action', 'additional', 'addressed', 'advanced', 'advantage', 'advantages', 'after', 'age', 'ai', 'aid', 'alan', 'algorithm', 'algorithms', 'alignment', 'alike', 'all', 'almost', 'along', 'alpac', 'already', 'also', 'although', 'ambiguous', 'among', 'amount', 'amounts', 'an', 'analysis', 'analyze', 'analyzed', 'and', 'annotated', 'annotation', 'annotations', 'another', 'answering', 'answers', 'apertium', 'apparent', 'applications', 'applied', 'apply', 'applying', 'approach', 'approaches', 'arabic', 'are', 'area', 'areas', 'art', 'article', 'articulated', 'artificial', 'as', 'aspects', 'assign', 'at', 'attaching', 'author', 'authors', 'automated', 'automatic', 'automatically', 'available', 'base', 'based', 'basque', 'be', 'became', 'become', 'been', 'before', 'began', 'behaviour', 'behind', 'being', 'below', 'between', 'beyond', 'big', 'blocks', 'both', 'branch', 'broadly', 'build', 'bulgarian', 'but', 'by', 'cache', 'called', 'calling', 'calls', 'can', 'canada', 'capable', 'capture', 'carbonell', 'care', 'cases', 'catalan', 'categories', 'categorize', 'centering', 'certainty', 'changes', 'chatterbots', 'chinese', 'chomskyan', 'claimed', 'classes', 'closely', 'coarse', 'coding', 'cognition', 'cognitive', 'collection', 'combination', 'combining', 'common', 'commonly', 'comparison', 'complex', 'complexity', 'component', 'comprehension', 'comprising', 'computational', 'computer', 'computers', 'computing', 'conceptual', 'concerned', 'conducted', 'conferences', 'confronted', 'conll', 'consider', 'construction', 'consuming', 'contained', 'containing', 'contains', 'content', 'contents', 'context', 'contexts', 'contextual', 'continue', 'continued', 'continues', 'convenience', 'corpora', 'corpus', 'corresponding', 'coupled', 'creating', 'criterion', 'cullingford', 'czech', 'danish', 'data', 'day', 'days', 'deal', 'decision', 'decisions', 'deep', 'defining', 'depended', 'dependency', 'designed', 'desired', 'despite', 'develop', 'developed', 'development', 'developmental', 'devising', 'dictionary', 'different', 'difficult', 'direct', 'directed', 'directions', 'directly', 'discouraged', 'distinct', 'division', 'do', 'documents', 'dominance', 'dramatically', 'drawback', 'due', 'during', 'dutch', 'each', 'earliest', 'early', 'effectively', 'effort', 'elaborate', 'electronic', 'elimination', 'eliza', 'embeddings', 'emotion', 'emphasizes', 'emulate', 'emulates', 'end', 'engineering', 'english', 'enormous', 'enough', 'entailed', 'entire', 'equation', 'erroneous', 'error', 'errors', 'especially', 'european', 'evaluation', 'eventually', 'example', 'examples', 'exceeded', 'existing', 'expectations', 'experience', 'experiment', 'explainability', 'explained', 'explicit', 'express', 'extract', 'extraction', 'extrapolate', 'extremely', 'fact', 'failed', 'feature', 'features', 'field', 'findings', 'first', 'five', 'flurry', 'focus', 'focused', 'following', 'for', 'form', 'found', 'frameworks', 'free', 'frequently', 'from', 'fulfill', 'fully', 'functional', 'funding', 'further', 'future', 'general', 'generally', 'generated', 'generation', 'generative', 'generic', 'george', 'georgetown', 'german', 'given', 'goal', 'gone', 'government', 'governmental', 'gracefully', 'gradual', 'grammar', 'grammars', 'great', 'greek', 'growth', 'had', 'hand', 'handling', 'handwritten', 'hard', 'has', 'have', 'head', 'health', 'healthcare', 'heavily', 'heritage', 'heuristic', 'hey', 'hidden', 'higher', 'historical', 'hours', 'how', 'however', 'hpsg', 'human', 'hungarian', 'hurts', 'ibm', 'idea', 'ideas', 'if', 'implemented', 'imply', 'importance', 'important', 'improve', 'in', 'inaccessible', 'include', 'included', 'including', 'increase', 'increases', 'increasing', 'increasingly', 'inference', 'inferior', 'information', 'inherent', 'input', 'insights', 'instance', 'instead', 'insufficient', 'integrated', 'intelligence', 'intelligent', 'intent', 'interaction', 'interactions', 'interdisciplinary', 'interest', 'intermediate', 'interpretability', 'interpretation', 'intertwined', 'into', 'introduced', 'introduction', 'involve', 'involved', 'involves', 'is', 'it', 'italian', 'its', 'jabberwacky', 'japanese', 'john', 'joseph', 'knowledge', 'lakoff', 'language', 'languages', 'large', 'largely', 'larger', 'late', 'latest', 'law', 'laws', 'learn', 'learning', 'led', 'lehnert', 'lesk', 'less', 'lessening', 'level', 'like', 'likewise', 'limit', 'limitation', 'limited', 'lines', 'linguistics', 'list', 'little', 'long', 'lookup', 'low', 'machine', 'machinery', 'made', 'mainstream', 'maintained', 'major', 'make', 'making', 'man', 'many', 'margie', 'mark', 'markov', 'matching', 'mathematical', 'may', 'meaning', 'means', 'measured', 'measures', 'medicine', 'meehan', 'mental', 'metaphor', 'metaphorically', 'methodology', 'methods', 'mid', 'might', 'mind', 'misspelled', 'model', 'modeling', 'models', 'moore', 'more', 'morphology', 'most', 'much', 'multilingual', 'multilinguality', 'multimodal', 'multimodality', 'multiple', 'my', 'natural', 'need', 'network', 'networks', 'neural', 'neuroscience', 'nevertheless', 'new', 'nlp', 'nmt', 'no', 'non', 'not', 'notable', 'notably', 'notes', 'notion', 'now', 'nuances', 'number', 'observed', 'obviating', 'obvious', 'occurred', 'of', 'offers', 'official', 'often', 'omitted', 'on', 'one', 'only', 'ontologies', 'operationalizable', 'operationalization', 'or', 'organize', 'other', 'others', 'otherwise', 'output', 'over', 'pam', 'paradigm', 'parliament', 'parry', 'parses', 'parsing', 'part', 'particular', 'patient', 'pcfg', 'period', 'person', 'perspective', 'phrase', 'phrasebook', 'physically', 'piece', 'pipeline', 'pipelines', 'plot', 'plural', 'politics', 'popular', 'popularity', 'portuguese', 'possible', 'possibly', 'postprocessing', 'potentially', 'power', 'practical', 'premise', 'preprocessing', 'presence', 'presented', 'probabilistic', 'problem', 'procedures', 'proceedings', 'process', 'processes', 'processing', 'produce', 'produced', 'produces', 'producing', 'program', 'programmers', 'progress', 'prone', 'proper', 'properties', 'proposed', 'provide', 'provided', 'provides', 'psycholinguistics', 'psychology', 'psychotherapist', 'published', 'pursued', 'qualm', 'quantitative', 'question', 'questions', 'racter', 'rarely', 'rather', 'raw', 'real', 'recent', 'recently', 'recognition', 'records', 'reduced', 'reference', 'refers', 'relative', 'relevant', 'reliable', 'relied', 'rely', 'relying', 'remain', 'replaced', 'report', 'representation', 'representations', 'represents', 'require', 'required', 'requires', 'research', 'researched', 'resource', 'responding', 'response', 'restricted', 'result', 'results', 'revived', 'revolution', 'rhetorical', 'rising', 'robust', 'rogerian', 'room', 'roots', 'rule', 'rules', 'russian', 'sam', 'say', 'schank', 'science', 'scientific', 'searle', 'see', 'seeking', 'seen', 'semantic', 'semantics', 'semi', 'senses', 'sentence', 'sentences', 'separate', 'sequence', 'series', 'serve', 'set', 'sets', 'shared', 'she', 'shift', 'shifted', 'should', 'showing', 'shrdlu', 'significant', 'similar', 'simply', 'simulation', 'since', 'sixty', 'slovenian', 'slower', 'small', 'smt', 'so', 'soft', 'solved', 'solving', 'some', 'sometimes', 'somewhat', 'sort', 'spanish', 'speaking', 'specifically', 'speech', 'standing', 'starting', 'startlingly', 'state', 'statistical', 'steady', 'stemming', 'steps', 'still', 'strong', 'structure', 'structured', 'structures', 'studies', 'study', 'style', 'subdivided', 'subfield', 'substantial', 'subtasks', 'success', 'successes', 'successful', 'successfully', 'such', 'summarized', 'supervised', 'supplying', 'swedish', 'symbolic', 'syntactic', 'system', 'systems', 'tagging', 'take', 'talespin', 'task', 'tasks', 'technical', 'technically', 'techniques', 'technology', 'ten', 'tend', 'term', 'terms', 'test', 'text', 'textual', 'than', 'that', 'the', 'them', 'themselves', 'then', 'theoretical', 'theories', 'theory', 'there', 'these', 'they', 'things', 'this', 'those', 'though', 'thought', 'three', 'through', 'thus', 'ties', 'time', 'titled', 'to', 'tokenization', 'tomorrow', 'topics', 'towards', 'training', 'trajectories', 'transformational', 'transformations', 'transforming', 'translation', 'transparency', 'tree', 'trees', 'trends', 'turing', 'turkish', 'turn', 'two', 'typical', 'typically', 'unannotated', 'under', 'underlies', 'underpinnings', 'understandable', 'understanding', 'unfamiliar', 'union', 'units', 'unmanageable', 'unsupervised', 'until', 'up', 'upon', 'uptake', 'usages', 'use', 'used', 'using', 'valued', 'various', 'very', 'viewed', 'vocabularies', 'was', 'weakly', 'web', 'weights', 'weizenbaum', 'well', 'were', 'what', 'when', 'where', 'whereas', 'which', 'while', 'whose', 'why', 'wide', 'widespread', 'wilensky', 'will', 'with', 'within', 'without', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worlds', 'would', 'write', 'writing', 'written', 'year', 'years', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "print(Word_Vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b8a5e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Features: 861\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Unique Features:\", len(Word_Vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac9fc4",
   "metadata": {},
   "source": [
    "# Congratulations, you have learned how to create a Bag of Words Model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
